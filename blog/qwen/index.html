<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Qwen | Qiyuan.Tech</title><meta name=keywords content><meta name=description content="4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme."><meta name=author content="Qiyuan.Tech"><link rel=canonical href=https://qiyuan-tech.github.io/blog/qwen/><link crossorigin=anonymous href=/assets/css/stylesheet.0ba7b7b31d6f65f4630a254e3d5ed3b35b42e349fc949c2df73c456e864b8350.css integrity="sha256-C6e3sx1vZfRjCiVOPV7Ts1tC40n8lJwt9zxFboZLg1A=" rel="preload stylesheet" as=style><link rel=icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=16x16 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=icon type=image/png sizes=32x32 href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg%22><link rel=apple-touch-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><link rel=mask-icon href=https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://qiyuan-tech.github.io/blog/qwen/><link rel=alternate hreflang=zh href=https://qiyuan-tech.github.io/zh/blog/qwen/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.f20a5212619392e989b6d24ad9ce42302014debfad4d3c8c01db030c36d03475.js integrity="sha256-8gpSEmGTkumJttJK2c5CMCAU3r+tTTyMAdsDDDbQNHU="></script><meta property="og:title" content="Introducing Qwen"><meta property="og:description" content="4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme."><meta property="og:type" content="article"><meta property="og:url" content="https://qiyuan-tech.github.io/blog/qwen/"><meta property="og:image" content="https://qiyuan-tech.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2024-01-23T22:13:29+08:00"><meta property="article:modified_time" content="2024-01-23T22:13:29+08:00"><meta property="og:site_name" content="Qiyuan.Tech"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://qiyuan-tech.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Qwen"><meta name=twitter:description content="4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.
PAPER GITHUB HUGGING FACE MODELSCOPE DISCORD
Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":2,"name":"Blog","item":"https://qiyuan-tech.github.io/blog/"},{"@type":"ListItem","position":3,"name":"Introducing Qwen","item":"https://qiyuan-tech.github.io/blog/qwen/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Qwen","name":"Introducing Qwen","description":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.\nPAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nAdditionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.","keywords":[],"articleBody":"4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.\nPAPER GITHUB HUGGING FACE MODELSCOPE DISCORD\nAdditionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.\nOverview In general, Qwen is more than a language model but a project towards AGI which for now consists of LLM and LMM. The following figure shows the main components of Qwen:\nwhere Qwen refers to the base language model, while Qwen-Chat refers to the chat model trained with techniques like SFT and RLHF. We also have models specialized for domains and tasks, such as Code-Qwen for coding and Math-Qwen for mathematics. LLM can be extended to multimodality with modality alignment, and thus we have vision-language model Qwen-VL as well as audio-language model Qwen-Audio. Note that this blog mainly serves for introducing the language model. As to the large multimodal models (LMM), such as Qwen-VL and Qwen-Audio, please refer to the respective blog.\nBase Model: A Good Starting Point for Alignment The general procedure of building an assistant model includes pretraining and post-training, where the latter mostly consists of SFT and RLHF. As to pretraining, similar to previous LLM, GPT-3, Llama, Qwen is a Transformer-based language model pretrained by the task of next token prediction. For simplicity and stability, we did not introduce more tasks for the language model but focus on model size scaling and data scaling. For now, we have developed 5 models of different sizes, 4 of which are opensourced. Specially, we now release Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B.\nModel Release Date Max Length System Prompt Enhancement # of Pretrained Tokens Minimum GPU Memory Usage of Finetuning (Q-Lora) Minimum GPU Usage of Generating 2048 Tokens (Int4) Tool Usage Qwen-1.8B 23.11.30 32K ✔ 2.2T 5.8GB 2.9GB ✔ Qwen-7B 23.08.03 32K ✘ 2.4T 11.5GB 8.2GB ✔ Qwen-14B 23.09.25 8K ✘ 3.0T 18.7GB 13.0GB ✔ Qwen-72B 23.11.30 32K ✔ 3.0T 61.4GB 48.9GB ✔ Models are sufficiently trained with 2-3 trillion tokens. The pretraining data are multilingual, and thus Qwen is essentially a multilingual model instead of a model of a single language or bilingual. Note that due to the limitations of our pretraining data, the model is strongly capable of English and Chinese and also capable of other languages, such as Spanish, French, and Japanese. To extend its multilingual capabilities, we applied a tokenizer with high efficiency in encoding information from different languages. In comparison with other tokenizers, ours demonstrates high compression rate in a series of languages.\nAnother focus of our pretraining is the extension of context length. We directly apply continual pretraining with longer context length and larger base value for RoPE. Additionally, we find that.this method is also effective in extrapolation. Now our opensourced models mostly support a context length of 32K tokens, and they were evaluated through L-Eval and “Needle in a Haystack”.\nModel Input Length Average Coursera GSM QuALITY TOEFL CodeU SFcition ChatGPT-3.5-16k 16K 60.73 63.51 84.00 61.38 78.43 12.22 64.84 Qwen-72B-Chat 32K 62.30 58.13 76.00 77.22 86.24 6.66 69.53 Benchmark evaluation shows that our largest opensourced model Qwen-72B as well as the largest proprietary shows competitive performance against Llama 2, GPT-3.5 and GPT-4.\nNote that this is an evaluation of base language model. This only reflects that we might have a good starting point for post-training, i.e., SFT and RLHF.\nAlignment We refer both techniques to the word “alignment” in post-training. Currently, it is consensus that we can obtain a chat model with a relatively small amount of finetuning data. We focus on improving the diversity and complexity (instag and tulu 2) of the SFT data and strictly control the quality by manual checking and automatic evaluation.\nBased on a good SFT model, we can then explore the effects of RLHF. It is difficult to train RLHF, specifically PPO-based method, Besides the training instabilities of PPO, another key to the final performance is the quality of reward model. Therefore, we have spent efforts in building a reliable reward model by reward model pretraining on large-scale comparison data and finetuning on carefully labeled comparison data of high quality. In comparison with the SFT model, we find that the RLHF model is more creative and follows the instructions better, and thus its generated responses are more preferred by human annotators.\nTool Use and Agent One of the most amazing parts of today’s LLMs is the capabilities of tool use and agent playing. We directly label data of ReAct formats in order to endow the abilities of generating thought and action and generating responses based on previous steps and observations. Also, the model directly learns the in-context learning ability and thus it then can use unseen tool through understanding instructions and demonstrations.\nWe currently support function calling, code interpreter, and hugging face agent, which respectively serves for tool use, data analysis and using AI models for different outputs, say image generation. Furthermore, based on our agent framework, we further build a project called AgentFabric, following GPTs, which allows you to build a specialzed AI agent for yourself simply by chatting with our model for configuration.\nSummary We release the Qwen series, and in this blog, we provide a simple introduction to the Qwen language models Now, we are still following the recipes of pretraining, SFT, and RLHF and we are figuring out a path towards scaling model and data. We hope that our opensource is contributive to the research and application communities.\n","wordCount":"949","inLanguage":"en","datePublished":"2024-01-23T22:13:29+08:00","dateModified":"2024-01-23T22:13:29+08:00","author":{"@type":"Person","name":"Qiyuan.Tech"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://qiyuan-tech.github.io/blog/qwen/"},"publisher":{"@type":"Organization","name":"Qiyuan.Tech","logo":{"@type":"ImageObject","url":"https://acd-assets.alicdn.com/acd_work/tongyi/assets/logo.svg"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="Qiyuan.Tech (Alt + H)"><svg width="25" height="24" viewBox="0 0 25 24" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"><defs><linearGradient x1="76.7202373%" y1="41.6070847%" x2="18.306123%" y2="65.5065085%" id="linearGradient-9_1k7ha4jv-1"><stop stop-color="#797beb" offset="0"/><stop stop-color="#373080" offset="100%"/></linearGradient></defs><g id="Symbols" stroke="none" stroke-width="1" fill="none" fill-rule="evenodd"><g id="_编组-7" fill="url(#linearGradient-9_1k7ha4jv-1)"><path d="M12.2746711.0C12.5125388.0 12.7434104.12583746 12.8693403.33556656l1.3852293 2.3769298h6.1425827C20.63502 2.71249636 20.8658915 2.83833382 20.9918215 3.04806292l1.7420308 2.97815322C22.8597822 6.23594524 22.8597822 6.5016021 22.7338523 6.7113312L22.6988718 6.76725896C22.6708873 6.80920478 22.6359068 6.8511506 22.5939301 6.88610545L21.2156969 9.24905331l3.050303 5.24322749L24.3429571 14.6041363C24.4339065 14.8138654 24.4548948 15.0795223 24.3919298 15.2682785l-1.6021086 2.7404602C22.6638912 18.2184678 22.4400158 18.3443053 22.195152 18.3443053L19.4176972 18.3582872 16.353402 23.6504515C16.227472 23.8601806 16.0035966 23.9860181 15.7587328 23.9860181L12.2886633 24H12.2396906C12.0158151 23.9860181 11.7989358 23.8601806 11.6869981 23.6644334l-1.490171-2.551704H4.13120166C4.0822289 21.1267113 4.04025225 21.1267113 3.9912795 21.1267113 3.75341183 21.1267113 3.52254028 21.0008739 3.39661034 20.7911448L1.79450165 18.0506845C1.66857171 17.8479464 1.66857171 17.5892805 1.79450165 17.3725604l1.37823324-2.3909117L.0944474553 9.69647539c-.1259299404-.20273813-.1259299404-.46140402.0-.678124089999999L1.80849387 6.02621614c.11193772-.2097291.34280928-.33556656.59466916-.33556656C2.466128 5.67666764 2.51510075 5.69064958 2.56407351 5.70463152H5.40449327L8.44080406.46140402C8.45479628.4194582 8.46179238.38450335 8.48278071.35653947L8.49677292.33556656C8.62270286.12583746 8.84657831.0 9.09144209.0H12.2816672 12.2746711zM9.04246933.72706088 5.74730256 6.41071949H2.3751786L8.93752771 17.6871541H5.59338819l-1.61610091 2.789397H10.5606247l1.6790659 2.8942616 6.5623491-11.3043985 1.6790659 2.8802797L23.7203035 14.9327119 20.4181406 9.24905331l1.6650737-2.8662977L9.00049268 6.39673755 10.6795586 3.51645791 9.04946544.72706088H9.04246933zM16.1435187 9.82930382 12.2187023 16.5755899 8.2938858 9.82930382h7.8496329z" id="_形状"/></g></g></svg></a><div class=logo-switches></div></div><ul id=menu><li><a href=/resources title=Resources><span>Resources</span></a></li><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Qwen</h1><div class=post-meta><span title='2024-01-23 22:13:29 +0800 +0800'>January 23, 2024</span>&nbsp;·&nbsp;5 min&nbsp;·&nbsp;949 words&nbsp;·&nbsp;Qiyuan.Tech&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://qiyuan-tech.github.io/zh/blog/qwen/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p>4 months after our first release of Qwen-7B, which is the starting point of our opensource journey of large language models (LLM), we now provide an introduction to the Qwen series to give you a whole picture of our work as well as our objectives. Below are important links to our opensource projects and community.</p><p><a href=https://arxiv.org/abs/2309.16609 class="btn external" target=_blank>PAPER</a>
<a href=https://github.com/QwenLM/Qwen class="btn external" target=_blank>GITHUB</a>
<a href=https://huggingface.co/Qwen class="btn external" target=_blank>HUGGING FACE</a>
<a href=https://modelscope.cn/organization/qwen class="btn external" target=_blank>MODELSCOPE</a>
<a href=https://discord.gg/z3GAxXZ9C class="btn external" target=_blank>DISCORD</a></p><p>Additionally, we have WeChat groups for chatting and we invite you to join the groups through the provided link in our GitHub readme.</p><iframe src=https://qwen-qwen-72b-chat-demo.hf.space frameborder=0 width=850 height=1000></iframe><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>In general, Qwen is more than a language model but a project towards AGI which for now consists of LLM and LMM. The following figure shows the main components of Qwen:</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen/family.png#center width=80%></figure><p>where Qwen refers to the base language model, while Qwen-Chat refers to the chat model trained with techniques like SFT and RLHF. We also have models specialized for domains and tasks, such as Code-Qwen for coding and Math-Qwen for mathematics. LLM can be extended to multimodality with modality alignment, and thus we have vision-language model Qwen-VL as well as audio-language model Qwen-Audio. Note that this blog mainly serves for introducing the language model. As to the large multimodal models (LMM), such as Qwen-VL and Qwen-Audio, please refer to the respective blog.</p><h2 id=base-model-a-good-starting-point-for-alignment>Base Model: A Good Starting Point for Alignment<a hidden class=anchor aria-hidden=true href=#base-model-a-good-starting-point-for-alignment>#</a></h2><p>The general procedure of building an assistant model includes pretraining and post-training, where the latter mostly consists of SFT and RLHF. As to pretraining, similar to previous LLM, GPT-3, Llama, Qwen is a Transformer-based language model pretrained by the task of next token prediction. For simplicity and stability, we did not introduce more tasks for the language model but focus on model size scaling and data scaling. For now, we have developed 5 models of different sizes, 4 of which are opensourced. Specially, we now release Qwen-1.8B, Qwen-7B, Qwen-14B, and Qwen-72B.</p><table><thead><tr><th>Model</th><th>Release Date</th><th>Max Length</th><th>System Prompt Enhancement</th><th># of Pretrained Tokens</th><th>Minimum GPU Memory Usage of Finetuning (Q-Lora)</th><th>Minimum GPU Usage of Generating 2048 Tokens (Int4)</th><th>Tool Usage</th></tr></thead><tbody><tr><td>Qwen-1.8B</td><td>23.11.30</td><td>32K</td><td>✔</td><td>2.2T</td><td>5.8GB</td><td>2.9GB</td><td>✔</td></tr><tr><td>Qwen-7B</td><td>23.08.03</td><td>32K</td><td>✘</td><td>2.4T</td><td>11.5GB</td><td>8.2GB</td><td>✔</td></tr><tr><td>Qwen-14B</td><td>23.09.25</td><td>8K</td><td>✘</td><td>3.0T</td><td>18.7GB</td><td>13.0GB</td><td>✔</td></tr><tr><td>Qwen-72B</td><td>23.11.30</td><td>32K</td><td>✔</td><td>3.0T</td><td>61.4GB</td><td>48.9GB</td><td>✔</td></tr></tbody></table><p>Models are sufficiently trained with 2-3 trillion tokens. The pretraining data are multilingual, and thus Qwen is essentially a multilingual model instead of a model of a single language or bilingual. Note that due to the limitations of our pretraining data, the model is strongly capable of English and Chinese and also capable of other languages, such as Spanish, French, and Japanese. To extend its multilingual capabilities, we applied a tokenizer with high efficiency in encoding information from different languages. In comparison with other tokenizers, ours demonstrates high compression rate in a series of languages.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen/tokenizer.png#center width=80%></figure><p>Another focus of our pretraining is the extension of context length. We directly apply continual pretraining with longer context length and larger base value for RoPE. Additionally, we find that.this method is also effective in extrapolation. Now our opensourced models mostly support a context length of 32K tokens, and they were evaluated through L-Eval and “Needle in a Haystack”.</p><table><thead><tr><th>Model</th><th>Input Length</th><th>Average</th><th>Coursera</th><th>GSM</th><th>QuALITY</th><th>TOEFL</th><th>CodeU</th><th>SFcition</th></tr></thead><tbody><tr><td>ChatGPT-3.5-16k</td><td>16K</td><td>60.73</td><td>63.51</td><td>84.00</td><td>61.38</td><td>78.43</td><td>12.22</td><td>64.84</td></tr><tr><td>Qwen-72B-Chat</td><td>32K</td><td>62.30</td><td>58.13</td><td>76.00</td><td>77.22</td><td>86.24</td><td>6.66</td><td>69.53</td></tr></tbody></table><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen/haystack.png#center width=80%></figure><p>Benchmark evaluation shows that our largest opensourced model Qwen-72B as well as the largest proprietary shows competitive performance against Llama 2, GPT-3.5 and GPT-4.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen/result.png#center width=80%></figure><p>Note that this is an evaluation of base language model. This only reflects that we might have a good starting point for post-training, i.e., SFT and RLHF.</p><h2 id=alignment>Alignment<a hidden class=anchor aria-hidden=true href=#alignment>#</a></h2><p>We refer both techniques to the word “alignment” in post-training. Currently, it is consensus that we can obtain a chat model with a relatively small amount of finetuning data. We focus on improving the diversity and complexity (instag and tulu 2) of the SFT data and strictly control the quality by manual checking and automatic evaluation.</p><p>Based on a good SFT model, we can then explore the effects of RLHF. It is difficult to train RLHF, specifically PPO-based method, Besides the training instabilities of PPO, another key to the final performance is the quality of reward model. Therefore, we have spent efforts in building a reliable reward model by reward model pretraining on large-scale comparison data and finetuning on carefully labeled comparison data of high quality. In comparison with the SFT model, we find that the RLHF model is more creative and follows the instructions better, and thus its generated responses are more preferred by human annotators.</p><figure><img src=https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/assets/blog/qwen/rlhf.png#center width=80%></figure><h2 id=tool-use-and-agent>Tool Use and Agent<a hidden class=anchor aria-hidden=true href=#tool-use-and-agent>#</a></h2><p>One of the most amazing parts of today’s LLMs is the capabilities of tool use and agent playing. We directly label data of ReAct formats in order to endow the abilities of generating thought and action and generating responses based on previous steps and observations. Also, the model directly learns the in-context learning ability and thus it then can use unseen tool through understanding instructions and demonstrations.</p><p>We currently support function calling, code interpreter, and hugging face agent, which respectively serves for tool use, data analysis and using AI models for different outputs, say image generation. Furthermore, based on our agent framework, we further build a project called AgentFabric, following GPTs, which allows you to build a specialzed AI agent for yourself simply by chatting with our model for configuration.</p><h2 id=summary>Summary<a hidden class=anchor aria-hidden=true href=#summary>#</a></h2><p>We release the Qwen series, and in this blog, we provide a simple introduction to the Qwen language models Now, we are still following the recipes of pretraining, SFT, and RLHF and we are figuring out a path towards scaling model and data. We hope that our opensource is contributive to the research and application communities.</p></div><footer class=post-footer><ul class=post-tags></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Introducing Qwen on twitter" href="https://twitter.com/intent/tweet/?text=Introducing%20Qwen&url=https%3a%2f%2fqiyuan-tech.github.io%2fblog%2fqwen%2f&hashtags="><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing Qwen on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fqiyuan-tech.github.io%2fblog%2fqwen%2f&title=Introducing%20Qwen"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing Qwen on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fqiyuan-tech.github.io%2fblog%2fqwen%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Introducing Qwen on telegram" href="https://telegram.me/share/url?text=Introducing%20Qwen&url=https%3a%2f%2fqiyuan-tech.github.io%2fblog%2fqwen%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://qiyuan-tech.github.io>Qiyuan.Tech</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a>. Website template from <a href=https://github.com/QwenLM/qwenlm.github.io rel="noopener noreferrer" target=_blank>Qwen</a>.</span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>